{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "3"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 1. Tools for text processing\n",
    "<p><img style=\"float: right ; margin: 5px 20px 5px 10px; width: 45%\" src=\"https://assets.datacamp.com/production/project_38/img/Moby_Dick_p510_illustration.jpg\"> </p>\n",
    "<p>What are the most frequent words in Herman Melville's novel, Moby Dick, and how often do they occur?</p>\n",
    "<p>In this notebook, we'll scrape the novel <em>Moby Dick</em> from the website <a href=\"https://www.gutenberg.org/\">Project Gutenberg</a> (which contains a large corpus of books) using the Python package <code>requests</code>. Then we'll extract words from this web data using <code>BeautifulSoup</code>. Finally, we'll dive into analyzing the distribution of words using the Natural Language ToolKit (<code>nltk</code>) and <code>Counter</code>.</p>\n",
    "<p>The <em>Data Science pipeline</em> we'll build in this notebook can be used to visualize the word frequency distributions of any novel that you can find on Project Gutenberg. The natural language processing tools used here apply to much of the data that data scientists encounter as a vast proportion of the world's data is unstructured data and includes a great deal of text.</p>\n",
    "<p>Let's start by loading in the three main Python packages we are going to use.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Instructions\n",
    "Import the four Python modules you'll use in this project:\n",
    "\n",
    "requests to fetch the html file that contains the book.\n",
    "BeautifulSoup, from the bs4 module, to extract the words from the html file.\n",
    "nltk to process our text.\n",
    "Counter, from the collections module, to analyze the frequncy of our processed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true,
    "dc": {
     "key": "3"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "10"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 2. Request Moby Dick\n",
    "<p>To analyze Moby Dick, we need to get the contents of Moby Dick from <em>somewhere</em>. Luckily, the text is freely available online at Project Gutenberg as an HTML file: https://www.gutenberg.org/files/2701/2701-h/2701-h.htm .</p>\n",
    "<p><strong>Note</strong> that HTML stands for Hypertext Markup Language and is the standard markup language for the web.</p>\n",
    "<p>To fetch the HTML file with Moby Dick we're going to use the <code>request</code> package to make a <code>GET</code> request for the website, which means we're <em>getting</em> data from it. This is what you're doing through a browser when visiting a webpage, but now we're getting the requested page directly into Python instead. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Instructions\n",
    "Request the Moby Dick HTML file.\n",
    "\n",
    "Get the following URL and assign it to r:\n",
    "https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\n",
    "Extract the text from r and assign it to html.\n",
    "Print out the first 2000 characters in html.\n",
    "For a guide to how you use requests to download a webpage check out the request kickstart guide.\n",
    "\n",
    "Note that the HTML file you are asked to request in this task is a cashed version of this file from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "dc": {
     "key": "10"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com\n",
      "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"GET /assets.datacamp.com/production/project_147/datasets/2701-h.htm HTTP/1.1\" 200 1501037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\r\n",
      "\r\n",
      "<!DOCTYPE html\r\n",
      "   PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\r\n",
      "   \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\" >\r\n",
      "\r\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\r\n",
      "  <head>\r\n",
      "    <title>\r\n",
      "      Moby Dick; Or the Whale, by Herman Melville\r\n",
      "    </title>\r\n",
      "    <style type=\"text/css\" xml:space=\"preserve\">\r\n",
      "\r\n",
      "    body { background:#faebd0; color:black; margin-left:15%; margin-right:15%; text-align:justify }\r\n",
      "    P { text-indent: 1em; margin-top: .25em; margin-bottom: .25em; }\r\n",
      "    H1,H2,H3,H4,H5,H6 { text-align: center; margin-left: 15%; margin-right: 15%; }\r\n",
      "    hr  { width: 50%; text-align: center;}\r\n",
      "    .foot { margin-left: 20%; margin-right: 20%; text-align: justify; text-indent: -3em; font-size: 90%; }\r\n",
      "    blockquote {font-size: 100%; margin-left: 0%; margin-right: 0%;}\r\n",
      "    .mynote    {background-color: #DDE; color: #000; padding: .5em; margin-left: 10%; margin-right: 10%; font-family: sans-serif; font-size: 95%;}\r\n",
      "    .toc       { margin-left: 10%; margin-bottom: .75em;}\r\n",
      "    .toc2      { margin-left: 20%;}\r\n",
      "    div.fig    { display:block; margin:0 auto; text-align:center; }\r\n",
      "    div.middle { margin-left: 20%; margin-right: 20%; text-align: justify; }\r\n",
      "    .figleft   {float: left; margin-left: 0%; margin-right: 1%;}\r\n",
      "    .figright  {float: right; margin-right: 0%; margin-left: 1%;}\r\n",
      "    .pagenum   {display:inline; font-size: 70%; font-style:normal;\r\n",
      "               margin: 0; padding: 0; position: absolute; right: 1%;\r\n",
      "               text-align: right;}\r\n",
      "    pre        { font-family: times new roman; font-size: 100%; margin-left: 10%;}\r\n",
      "\r\n",
      "    table      {margin-left: 10%;}\r\n",
      "\r\n",
      "a:link {color:blue;\r\n",
      "\t\ttext-decoration:none}\r\n",
      "link {color:blue;\r\n",
      "\t\ttext-decoration:none}\r\n",
      "a:visited {color:blue;\r\n",
      "\t\ttext-decoration:none}\r\n",
      "a:hover {color:red}\r\n",
      "\r\n",
      "</style>\r\n",
      "  </head>\r\n",
      "  <body>\r\n",
      "<pre xml:space=\"preserve\">\r\n",
      "\r\n",
      "The Project Gutenberg EBook of Moby Dick; or The Whale, by Herman Melville\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywh\n"
     ]
    }
   ],
   "source": [
    "# Getting the Moby Dick HTML \n",
    "r = requests.get(' https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm')\n",
    "\n",
    "# Setting the correct text encoding of the HTML page\n",
    "r.encoding = 'utf-8'\n",
    "\n",
    "# Extracting the HTML from the request object\n",
    "html = r.text\n",
    "\n",
    "print(html[0:2000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "17"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 3. Get the text from the HTML\n",
    "<p>This HTML is not quite what we want. However, it does <em>contain</em> what we want: the text of <em>Moby Dick</em>. What we need to do now is <em>wrangle</em> this HTML to extract the text of the novel. For this we'll use the package <code>BeautifulSoup</code>.</p>\n",
    "<p>Firstly, a word on the name of the package: Beautiful Soup? In web development, the term \"tag soup\" refers to structurally or syntactically incorrect HTML code written for a web page. What Beautiful Soup does best is to make tag soup beautiful again and to extract information from it with ease! In fact, the main object created and queried when using this package is called <code>BeautifulSoup</code>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Instructions\n",
    "Extract the text from the html version of the book.\n",
    "\n",
    "Create a BeautifulSoup object from html as assign it to soup .\n",
    "Extract the text from the soup and assign it to text.\n",
    "Print out the text starting from character number 32000 until character number 34000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "dc": {
     "key": "17"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r which the beech tree\r\n",
      "        extended its branches.” —Darwin’s Voyage of a Naturalist.\r\n",
      "      \n",
      "\r\n",
      "        “‘Stern all!’ exclaimed the mate, as upon turning his head, he saw the\r\n",
      "        distended jaws of a large Sperm Whale close to the head of the boat,\r\n",
      "        threatening it with instant destruction;—‘Stern all, for your\r\n",
      "        lives!’” —Wharton the Whale Killer.\r\n",
      "      \n",
      "\r\n",
      "        “So be cheery, my lads, let your hearts never fail, While the bold\r\n",
      "        harpooneer is striking the whale!” —Nantucket Song.\r\n",
      "      \n",
      "\r\n",
      "     “Oh, the rare old Whale, mid storm and gale\r\n",
      "     In his ocean home will be\r\n",
      "     A giant in might, where might is right,\r\n",
      "     And King of the boundless sea.”\r\n",
      "      —Whale Song.\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "      CHAPTER 1. Loomings.\r\n",
      "    \n",
      "\r\n",
      "      Call me Ishmael. Some years ago—never mind how long precisely—having\r\n",
      "      little or no money in my purse, and nothing particular to interest me on\r\n",
      "      shore, I thought I would sail about a little and see the watery part of\r\n",
      "      the world. It is a way I have of driving off the spleen and regulating the\r\n",
      "      circulation. Whenever I find myself growing grim about the mouth; whenever\r\n",
      "      it is a damp, drizzly November in my soul; whenever I find myself\r\n",
      "      involuntarily pausing before coffin warehouses, and bringing up the rear\r\n",
      "      of every funeral I meet; and especially whenever my hypos get such an\r\n",
      "      upper hand of me, that it requires a strong moral principle to prevent me\r\n",
      "      from deliberately stepping into the street, and methodically knocking\r\n",
      "      people’s hats off—then, I account it high time to get to sea as soon\r\n",
      "      as I can. This is my substitute for pistol and ball. With a philosophical\r\n",
      "      flourish Cato throws himself upon his sword; I quietly take to the ship.\r\n",
      "      There is nothing surprising in this. If they but knew it, almost all men\r\n",
      "      in their degree, some time or other, cherish very nearly the same feelings\r\n",
      "      towards the ocean with me.\r\n",
      "    \n",
      "\r\n",
      "      Ther\n"
     ]
    }
   ],
   "source": [
    "# Creating a BeautifulSoup object from the HTML\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Getting the text out of the soup\n",
    "text = soup.get_text()\n",
    "print(text[32000:34000])\n",
    "# Printing out text between characters 32000 and 34000\n",
    "# ... YOUR CODE FOR TASK 3 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "24"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 4. Extract the words\n",
    "<p>We now have the text of the novel! There is some unwanted stuff at the start and some unwanted stuff at the end. We could remove it, but this content is so much smaller in amount than the text of Moby Dick that, to a first approximation, it is okay to leave it in.</p>\n",
    "<p>Now that we have the text of interest, it's time to count how many times each word appears, and for this we'll use <code>nltk</code> – the Natural Language Toolkit. We'll start by tokenizing the text, that is, remove everything that isn't a word (whitespace, punctuation, etc.) and then split the text into a list of words.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Instructions\n",
    "Tokenize the Moby Dick text.\n",
    "\n",
    "Initialize a regex tokenizer object tokenizer using nltk.tokenize.RegexpTokenizer, passing in a regular expression that will split the text into individual words.\n",
    "Use the correct method of your new tokenizer object to tokenize text and assign the resulting list of words to tokens.\n",
    "Print out the first 8 words / tokens.\n",
    "For how to use the nltk.tokenize.RegexpTokenizer function, please see the example in the nltk documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "dc": {
     "key": "24"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Moby', 'Dick', 'Or', 'the', 'Whale', 'by', 'Herman', 'Melville']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a tokenizer\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "# Tokenizing the text\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# Printing out the first 8 words / tokens \n",
    "tokens[0:8]\n",
    "# ... YOUR CODE FOR TASK 4 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "31"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 5. Make the words lowercase\n",
    "<p>OK! We're nearly there. Note that in the above 'Or' has a capital 'O' and that in other places it may not, but both 'Or' and 'or' should be counted as the same word. For this reason, we should build a list of all words in <em>Moby Dick</em> in which all capital letters have been made lower case.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Instructions\n",
    "Convert the words / tokens to lowercase.\n",
    "\n",
    "Loop through the words in tokens, make them lowercase, and store them in a list called words.\n",
    "Print out the first 8 words to make sure they are all lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "dc": {
     "key": "31"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list called words containing all tokens transformed to lower-case\n",
    "words=[token.lower()for token in tokens]\n",
    "\n",
    "# Printing out the first 8 words / tokens \n",
    "words[0:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "38"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 6. Load in stop words\n",
    "<p>It is common practice to remove words that appear a lot in the English language such as 'the', 'of' and 'a' because they're not so interesting. Such words are known as <em>stop words</em>. The package <code>nltk</code> includes a good list of stop words in English that we can use.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6: Instructions\n",
    "Load in the English stop words.\n",
    "\n",
    "Load in the English stop words from nltk and assign them to sw.\n",
    "Print out the first 8 stop words in sw.\n",
    "See the nltk documentation for how to load in the stop words.\n",
    "\n",
    "Before being able to load in the stop words, you have to download them using the command:\n",
    "\n",
    "nltk.download('stopwords')\n",
    "But in this project, this step has already been done for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "dc": {
     "key": "38"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the English stop words from nltk\n",
    "sw =  nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Printing out the first eight stop words\n",
    "sw[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "45"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 7. Remove stop words in Moby Dick\n",
    "<p>We now want to create a new list with all <code>words</code> in Moby Dick, except those that are stop words (that is, those words listed in <code>sw</code>).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 7: Instructions\n",
    "Create a new list with the words from Moby Dick, where stop words have been removed.\n",
    "\n",
    "Create a list words_ns that contains all words that are in words but not in sw.\n",
    "Print out the five first words in words_ns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "dc": {
     "key": "45"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moby', 'dick', 'whale', 'herman', 'melville']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list words_ns containing all words that are in words but not in sw\n",
    "words_ns=[word for word in words if word not in sw]\n",
    "# Printing the first 5 words_ns to check that stop words are gone\n",
    "words_ns[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "52"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 8. We have the answer\n",
    "<p>Our original question was:</p>\n",
    "<blockquote>\n",
    "  <p>What are the most frequent words in Herman Melville's novel Moby Dick and how often do they occur?</p>\n",
    "</blockquote>\n",
    "<p>We are now ready to answer that! Let's answer this question using the <code>Counter</code> class we imported earlier.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 8: Instructions\n",
    "Inintialize a Counter object called count using our words_ns list.\n",
    "Use the corresponding method to return the 10 most common words and their counts, assigning the result to top_ten.\n",
    "Print top_ten.\n",
    "Note: This step was recently updated. If you are receiving feedback related to the variable freqdist, you are likely using an outdated notebook. This can be avoided by refreshing the notebook (and losing all your progress), or by adding the following code to your solution:\n",
    "\n",
    "freqdist = nltk.FreqDist(words_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "dc": {
     "key": "52"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('whale', 1246), ('one', 925), ('like', 647), ('upon', 568), ('man', 527), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 452)]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Counter object from our processed list of words\n",
    "count = Counter(words_ns)\n",
    "\n",
    "# Store 10 most common words and their counts as top_ten\n",
    "top_ten = count.most_common(10)\n",
    "\n",
    "# Print the top ten words and their counts\n",
    "print(top_ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "59"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 9. The most common word\n",
    "<p>Nice! Using our variable <code>top_ten</code>, we now have an answer to our original question.</p>\n",
    "<p>The natural language processing skills we used in this notebook are also applicable to much of the data that Data Scientists encounter as the vast proportion of the world's data is unstructured data and includes a great deal of text. </p>\n",
    "<p>So, what word turned out to (<em>not surprisingly</em>) be the most common word in Moby Dick?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true,
    "dc": {
     "key": "59"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# What's the most common word in Moby Dick?\n",
    "most_common_word = 'whale'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
